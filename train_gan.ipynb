{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python2\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "from getpass import getuser\n",
    "from distutils.dir_util import copy_tree\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import config\n",
    "import lasagne\n",
    "from lasagne.regularization import regularize_network_params\n",
    "from lasagne.objectives import binary_crossentropy\n",
    "\n",
    "from iterator import Iterator\n",
    "import models_v2 as models\n",
    "\n",
    "\n",
    "\n",
    "import PIL.Image as Image\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training hyper-parameters\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0#TODO :NOT implemented yet!!\n",
    "num_epochs = 5\n",
    "max_patience = 100\n",
    "#data_augmentation={}\n",
    "savepath = 'save_models' #might need to change a bit\n",
    "loadpath = 'load_models'\n",
    "batch_size = 128\n",
    "extract_center = False\n",
    "load_caption = False\n",
    "nb_discriminator_steps = 2\n",
    "\n",
    "#Model Hyperparameters\n",
    "conv_before_pool = [1,1,1,1]\n",
    "n_filters = 64\n",
    "filter_size = 3\n",
    "n_units_dense_layer = 10\n",
    "out_nonlin = lasagne.nonlinearities.sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m The following folder already exists save_models\\gan. It will be overwritten in a few seconds...\u001b[0m\n",
      "Saving directory : save_models\\gan\n",
      "Loading directory : load_models\\gan\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# Saving path and stuff\n",
    "######################\n",
    "exp_name = 'gan'\n",
    "\n",
    "savepath=os.path.join(sys.path[0],savepath, exp_name)\n",
    "loadpath=os.path.join(sys.path[0],loadpath, exp_name)\n",
    "\n",
    "if not os.path.exists(savepath):\n",
    "    os.makedirs(savepath)\n",
    "else:\n",
    "    print('\\033[93m The following folder already exists {}. '\n",
    "          'It will be overwritten in a few seconds...\\033[0m'.format(\n",
    "              savepath))\n",
    "print 'Saving directory : ' + savepath\n",
    "print 'Loading directory : '+ loadpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loading validation data...\n",
      "Batch. train: 646, val 316, test 0\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "#Build dataset iterator\n",
    "#######################\n",
    "\n",
    "print \"Loading training data...\" #threads???\n",
    "train_iter = Iterator(which_set='train', batch_size = batch_size,\n",
    "            extract_center = extract_center, load_caption = load_caption)\n",
    "print \"Loading validation data...\" #threads???\n",
    "valid_iter = Iterator(which_set='valid', batch_size = batch_size,\n",
    "            extract_center = extract_center, load_caption = load_caption)\n",
    "test_iter = None\n",
    "\n",
    "n_batches_train = train_iter.n_batches\n",
    "n_batches_valid = valid_iter.n_batches\n",
    "n_batches_test = test_iter.n_batches if test_iter is not None else 0\n",
    "\n",
    "print \"Batch. train: %d, val %d, test %d\" % (n_batches_train,\n",
    "                            n_batches_valid, n_batches_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining symbolic variables and building models\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# Build generator and discriminator models\n",
    "##########################################\n",
    "print('Defining symbolic variables and building models')\n",
    "\n",
    "#Input and target var for the generator\n",
    "G_input_var = T.tensor4('generator_input_var') #m noise samples\n",
    "D_input_var = T.tensor4('true images')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gan model\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# Build generator and discriminator models\n",
    "##########################################\n",
    "\n",
    "print('Building gan model')\n",
    "gan = models.gan(G_input_var = G_input_var, D_input_var = D_input_var)\n",
    "\n",
    "#Print layers and shape (to debug)\n",
    "# print 'Generator layers'\n",
    "# for layer in lasagne.layers.get_all_layers(generator):\n",
    "#     print layer, layer.output_shape\n",
    "# print 'Discriminator layers'\n",
    "# for layer in lasagne.layers.get_all_layers(discriminator):\n",
    "#     print layer, layer.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining and compiling theano functions\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# Define and compile theano functions\n",
    "#####################################\n",
    "print \"Defining and compiling theano functions\"\n",
    "\n",
    "#Prediction functions\n",
    "#G_predictions = lasagne.layers.get_output(gan.G['last_layer'])\n",
    "D_predictions = lasagne.layers.get_output(gan.D['last_layer'])\n",
    "D_over_G_predictions = lasagne.layers.get_output(gan.D_over_G['last_layer'])\n",
    "\n",
    "#Loss functions\n",
    "D_loss = -(T.log(D_predictions) + T.log(1- D_over_G_predictions)).mean()\n",
    "G_loss = (T.log(1-D_over_G_predictions)).mean() #binary_crossentropy(D_predictions, D_target_var)\n",
    "#Ainsi, on fera de la descente de gradient sur chacune de ces loos\n",
    "\n",
    "#TODO : weight decay\n",
    "# if weight_decay > 0:\n",
    "#     gen_weightsl2 = regularize_network_params(\n",
    "#         generator, lasagne.regularization.l2)\n",
    "#     discr_weightsl2 = regularize_network_params(\n",
    "#         discriminator, lasagne.regularization.l2)\n",
    "#     gen_loss += weight_decay * gen_weightsl2\n",
    "#     discr_loss += weight_decay * discr_weightsl2\n",
    "\n",
    "\n",
    "#Update parameters for each model\n",
    "\n",
    "D_params = lasagne.layers.get_all_params(gan.D['last_layer'], trainable=True)\n",
    "D_updates = lasagne.updates.adam(D_loss, D_params, learning_rate = learning_rate)\n",
    "#discr_acc = $dicriminator accuracy\n",
    "D_train_fn = theano.function([G_input_var,D_input_var], D_loss, updates = D_updates)\n",
    "\n",
    "\n",
    "\n",
    "G_params = lasagne.layers.get_all_params(gan.G['last_layer'], trainable=True)\n",
    "G_updates = lasagne.updates.adam(G_loss, G_params, learning_rate = learning_rate)\n",
    "#gen_acc = #generator accuracy\n",
    "G_train_fn = theano.function([G_input_var], G_loss, updates=G_updates)\n",
    "\n",
    "print 'Done'\n",
    "#If ImportError: DLL load failed: The specified module could not be found\n",
    "#Open with ipython notebook instead of jupyter notebook (that may fix the problem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'yay'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.909804\n"
     ]
    }
   ],
   "source": [
    "print img_train_batch[0][0][7][32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "nb_discriminator_steps = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #Train discriminator\n",
    "    print 'Train discriminaTOR'\n",
    "    for k in range(nb_discriminator_steps):\n",
    "        \n",
    "        for i,batch in enumerate(train_iter):\n",
    "            if i!=0:\n",
    "                break\n",
    "            img_train_batch, center_train_batch, caps_train_batch = batch\n",
    "            img_train_batch, center_train_batch, caps_train_batch = img_train_batch[:10], center_train_batch[:10], caps_train_batch[:10]\n",
    "            img_train_batch = np.transpose(img_train_batch, (0,3,1,2))\n",
    "            center_train_batch = np.transpose(center_train_batch, (0,3,1,2))\n",
    "            #print img_train_batch.shape, center_train_batch.shape, np.array(list(caps_train_batch)).shape\n",
    "            \n",
    "            G_input_noise = np.random.rand(img_train_batch.shape[0],n_filters*8,4,4)\n",
    "            #print 'noise input var done'\n",
    "            D_cost_train_batch = D_train_fn(G_input_noise,center_train_batch)\n",
    "            print 'epoch = ', epoch , 'k = ', k, 'i = ', i, 'pixel' , img_train_batch[0][0][7][32], 'cost = ', D_cost_train_batch\n",
    "         \n",
    "    #Train generator    \n",
    "    print 'Train generator'\n",
    "    for i,batch in enumerate(train_iter):\n",
    "        if i!=0:\n",
    "            break\n",
    "            \n",
    "        G_input_noise = np.random.rand(128,n_filters*8,4,4)\n",
    "        #print 'noise input var done'\n",
    "        G_cost_train_batch = G_train_fn(G_input_noise)\n",
    "        print 'epoch = ', epoch , 'k = ', k, 'i = ', i, 'pixel' , img_train_batch[0][0][7][32], 'cost = ', G_cost_train_batch\n",
    "            \n",
    "print 'done'\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f0c65e2d0912>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "batch[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126L, 64L, 64L, 3L)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_train_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
